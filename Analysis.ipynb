{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8d6d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scikits.bootstrap as boot\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from math import isnan\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "\n",
    "# view all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['font.sans-serif'] = 'Spica Neue'\n",
    "#plt.rcParams['font.sans-serif'] = 'IPAexGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# network analysis #\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2fd34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file path\n",
    "def dates2fpaths(pkl_dir, start_date_str, end_dates_str):\n",
    "    start_date = pd.to_datetime(start_date_str).tz_localize(None)\n",
    "    end_date = pd.to_datetime(end_dates_str).tz_localize(None)\n",
    "    diff = (end_date - start_date).days + 1\n",
    "    dt_list = [start_date + timedelta(days=i) for i in range(diff)]\n",
    "    return [pkl_dir.joinpath(str(dt)[0:10].replace('-','')+'.pkl.xz') for dt in dt_list]\n",
    "\n",
    "# capture statistic information of vaccine texts\n",
    "def dataset_summary_vaccine(pkl_dir, fpaths):\n",
    "    # initialize variables\n",
    "    tweet_num =0\n",
    "    retweet_num =0\n",
    "    reply_num =0\n",
    "    favorite_count =0\n",
    "    user_name = []\n",
    "    user_hashtag = []\n",
    "    \n",
    "    for f in fpaths:\n",
    "        try:\n",
    "            dt = pd.to_datetime(str(f.name)[0:8])\n",
    "            print(dt)\n",
    "            df = pd.read_pickle(f)\n",
    "\n",
    "            # English only\n",
    "            df = df[(df['lang'] == 'en')] \n",
    "            df = df[df['text'].str.contains('vaccines|vaccine|vaccinated|vaccination|vaccineoutside|vaccinate|vaccinologist|vaccinert|coronavirusvaccine', na=False)]\n",
    "            \n",
    "            # number of retweet\n",
    "            day_retweet = len(df[df['retweeted_status_created_at'].notnull()])\n",
    "            retweet_num = retweet_num + day_retweet\n",
    "            \n",
    "            # number of reply\n",
    "            day_reply = len(df[df['in_reply_to_user_id_str'].notnull()])  \n",
    "            reply_num = reply_num + day_reply           \n",
    "            \n",
    "            # number of tweet\n",
    "            tweet_num = tweet_num + len(df)- day_retweet - day_reply\n",
    "            \n",
    "            # number of favourite\n",
    "            for i in range(len(df['favorite_count'])):\n",
    "                favorite_count += df['favorite_count'][i]\n",
    "            \n",
    "            # all accounts\n",
    "            for i in range(len(df['user_screen_name'])):\n",
    "                user_name.append(df['user_screen_name'][i])\n",
    "            \n",
    "            # all hashtags\n",
    "            for i in df['hashtags']:\n",
    "                # tokenize\n",
    "                tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "                new_words = tokenizer.tokenize(i)\n",
    "                for j in new_words:\n",
    "                    user_hashtag.append(j)\n",
    "                    \n",
    "        except:\n",
    "            print('error: ', f)\n",
    "    return tweet_num, retweet_num, reply_num, favorite_count, user_name, user_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b1333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retweet network\n",
    "# contruct retweet network\n",
    "def retweet_netwroks(pkl_dir, fpaths, filename):\n",
    "\n",
    "    e = []\n",
    "    for f in fpaths:\n",
    "        try:\n",
    "            dt = pd.to_datetime(str(f.name)[0:8])\n",
    "            print(dt)\n",
    "            df = pd.read_pickle(f)\n",
    "\n",
    "            # English only\n",
    "            df = df[(df['lang'] == 'en')] \n",
    "            \n",
    "            # retweets only\n",
    "            df = df[df['retweeted_status_created_at'].notnull()]\n",
    "\n",
    "            # find keyword cocurrence        \n",
    "            df = df[df['text'].str.contains('vaccines|vaccine|vaccinated|vaccination|vaccineoutside|vaccinate|vaccinologist|vaccinert|coronavirusvaccine', na=False)]\n",
    "            \n",
    "            # retweet edges\n",
    "            # from (source) and to (target or user who post the original)\n",
    "            edge_dic = df.groupby(by=['user_screen_name', 'retweeted_status_screen_name']).size().to_dict()\n",
    "            for k, v in edge_dic.items():\n",
    "                e.append([k[0], k[1], {'weight': v}])\n",
    "        except:\n",
    "            print('error: ', f)\n",
    "        \n",
    "    D = nx.from_edgelist(e, create_using=nx.DiGraph)\n",
    "    nx.write_gexf(D, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064ca4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Louvain algorithm in Gephi to find Pro and Anti clusters.\n",
    "# Download nodes information of each cluster by Gephi named \"RTnet_vacc_2021_06_30>20.csv\" \n",
    "# which modularity_class of '4' represent for Pro and '0' represent for Anti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc258a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network measures(including basic information of network, Network density, Clustering coefficient and Average distance)\n",
    "def retweet_network_information(name):\n",
    "    # RT network\n",
    "    G = nx.read_gexf(name)\n",
    "    print('All nodes:', nx.info(G))\n",
    "    print('\\n')\n",
    "\n",
    "    # RT network >= 20\n",
    "    node = list(G.nodes)\n",
    "    for i in range(G.number_of_nodes()):\n",
    "        if G.degree[node[i]] < 20:\n",
    "            G.remove_node(node[i])\n",
    "    print('node degree>=20:', nx.info(G))\n",
    "    \n",
    "    # save gexf\n",
    "    # nx.write_gexf(G, 'RTnet_vacc_2020_12_01 - 2021_06_30>20.gexf')\n",
    "\n",
    "    # nodes.csv from Gephi\n",
    "    nodes = pd.read_csv('RTnet_vacc_2021_06_30>20.csv')\n",
    "\n",
    "    # read nodes for Anti and Pro\n",
    "    # 4 represent for Pro, 0 represent for Anti \n",
    "    pro_nodes = list(nodes[nodes['modularity_class']==4]['Id'])\n",
    "    anti_nodes = list(nodes[nodes['modularity_class']==0]['Id'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('number of Pro: ', len(pro_nodes))\n",
    "    print('number of Anti: ',len(anti_nodes))\n",
    "    print('\\n')\n",
    "\n",
    "    # fliter nodes for pro\n",
    "    G_pro = G.subgraph(pro_nodes)\n",
    "    print(nx.info(G_pro))\n",
    "\n",
    "    print('\\n')\n",
    "    # fliter nodes for anti\n",
    "    G_anti = G.subgraph(anti_nodes)\n",
    "    print(nx.info(G_anti))\n",
    "\n",
    "    print('\\n')\n",
    "    # network density\n",
    "    G_network_density = nx.density(G)\n",
    "    print('G_network_density: ', G_network_density)\n",
    "\n",
    "    G_pro_network_density = nx.density(G_pro)\n",
    "    print('G_pro_network_density: ', G_pro_network_density)\n",
    "\n",
    "    G_anti_network_density = nx.density(G_anti)\n",
    "    print('G_anti_network_density: ', G_anti_network_density)\n",
    "    print('\\n')\n",
    "    \n",
    "    # global clustering coeffcient\n",
    "\n",
    "    G_average_clustering = nx.average_clustering(G)\n",
    "    G_pro_average_clustering = nx.average_clustering(G_pro)\n",
    "    G_anti_average_clustering = nx.average_clustering(G_anti)\n",
    "\n",
    "    print('average local clustering coefficient of entirety: ', G_average_clustering)\n",
    "    print('average local clustering coefficient of Pro: ', G_pro_average_clustering)\n",
    "    print('average local clustering coefficient of Anti: ', G_anti_average_clustering)\n",
    "    print('\\n')\n",
    "    \n",
    "    # average distance\n",
    "    G_pro_average_distance = nx.average_shortest_path_length(nx.to_undirected(G_pro))\n",
    "    G_anti_average_distance = nx.average_shortest_path_length(nx.to_undirected(G_anti))\n",
    "\n",
    "    print('Average distance of Pro: ', G_pro_average_distance)\n",
    "    print('Average distance of Anti: ', G_anti_average_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08196b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Top-10 users \n",
    "def find_top_users(name):\n",
    "    G = nx.read_gexf(name)\n",
    "    a = G.degree\n",
    "    degree = sorted(G.degree(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    def find_top_users_for_cluster(number):\n",
    "\n",
    "        node  = list(nodes[nodes['modularity_class']==number]['Id'])\n",
    "        num = len(list(nodes[nodes['modularity_class']==number]['Id']))\n",
    "\n",
    "        # names and degrees of all nodes labeled with 'number'\n",
    "        degree0  = []\n",
    "        for i in range(len(degree)):\n",
    "            if degree[i][0] in node:\n",
    "                degree0.append(degree[i])\n",
    "        ratio = num/len(nodes)*100\n",
    "        ratio=('%.2f' % ratio)\n",
    "\n",
    "        # 打印top 10 \n",
    "        top_10 = sorted(degree0, key=lambda x:x[1], reverse=True)[:10]\n",
    "        print(\"Top 10 of {}:\".format(number))\n",
    "        # print('Ratio:{}%'.format(ratio))\n",
    "        print(top_10)  \n",
    "\n",
    "    # print ratio and top user of each cluster\n",
    "    cluster_name = [4,0]\n",
    "    for i in cluster_name:\n",
    "        find_top_users_for_cluster(i)\n",
    "        print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f95b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Distribution_of_the_indegree(name):\n",
    "    \n",
    "    # RT network\n",
    "    G = nx.read_gexf(name)\n",
    "\n",
    "    # nodes.csv from Gephi\n",
    "    nodes = pd.read_csv('RTnet_vacc_2021_06_30>20.csv')\n",
    "\n",
    "    # read nodes for Anti and Pro\n",
    "    # 4 represent for Pro, 0 represent for Anti \n",
    "    pro_nodes = list(nodes[nodes['modularity_class']==4]['Id'])\n",
    "    anti_nodes = list(nodes[nodes['modularity_class']==0]['Id'])\n",
    "    \n",
    "    G_pro = G.subgraph(pro_nodes)\n",
    "    G_anti = G.subgraph(anti_nodes)\n",
    "    \n",
    "    ax1 = plt.subplot(2, 1, 1)\n",
    "    degree_hist = np.array(nx.degree_histogram(G_pro))\n",
    "    pk = degree_hist / degree_hist.sum()\n",
    "    plt.xlabel('In Degree of G_pro')\n",
    "    plt.ylabel('Fraction of Nodes')\n",
    "    plt.loglog(pk)\n",
    "    \n",
    "    ax2 = plt.subplot(2, 1, 2)  \n",
    "    degree_hist = np.array(nx.degree_histogram(G_anti))\n",
    "    pk = degree_hist / degree_hist.sum()\n",
    "    plt.xlabel('In Degree of G_anti')\n",
    "    plt.ylabel('Fraction of Nodes')\n",
    "    plt.loglog(pk)\n",
    "    \n",
    "    plt.show()\n",
    "    #plt.xlim(1, 11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2d2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "#fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-02-20', '2021-06-30')\n",
    "# capture statistic information of vaccine\n",
    "tweet_num_vaccine, retweet_num_vaccine, reply_num_vaccine, favorite_count_vaccine, user_name_vaccine, user_hashtag_vaccine = dataset_summary_vaccine(pkl_dir, fpaths)\n",
    "print(tweet_num_vaccine, retweet_num_vaccine, reply_num_vaccine, favorite_count_vaccine, len(set(user_name_vaccine)), len(set(user_hashtag_vaccine)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782661b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save retweet network\n",
    "\n",
    "# contruct retweet network\n",
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "# fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-02-20', '2021-06-30')\n",
    "retweet_netwroks(pkl_dir, fpaths, 'RTnet_vacc_2021_06_30>20.gexf')\n",
    "\n",
    "# contruct retweet network before mass vaccination\n",
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "#fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-02-20', '2020-12-01')\n",
    "retweet_netwroks(pkl_dir, fpaths, 'RTnet_vacc_2020_02_20 - 2020_12_01.gexf')\n",
    "\n",
    "# contruct retweet network after mass vaccination\n",
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "#fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-12-02', '2021-06-30')\n",
    "retweet_netwroks(pkl_dir, fpaths, 'RTnet_vacc_2020_12_01 - 2021_06_30.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures and find Top-10 users for different retweet network which 2020_12_01 represent the timepoint for mass vaccination \n",
    "# 2020_02_20 To 2021_06_30\n",
    "print('Timeline: From 2020_02_20 To 2021_06_30')\n",
    "retweet_network_information('RTnet_vacc_2021_06_30.gexf')\n",
    "find_top_users('RTnet_vacc_2021_06_30>20.gexf')\n",
    "\n",
    "# 2020_02_20 To 2020_12_01\n",
    "print('Timeline: From 2020_02_20 To 2020_12_01')\n",
    "retweet_network_information('RTnet_vacc_2020_02_20 - 2020_12_01.gexf')\n",
    "find_top_users('RTnet_vacc_2020_02_20 - 2020_12_01>20.gexf')\n",
    "\n",
    "# 2020_12_01 To 2021_06_30\n",
    "print('Timeline: From 2020_12_01 To 2021_06_30')\n",
    "retweet_network_information('RTnet_vacc_2020_12_01 - 2021_06_30.gexf')\n",
    "find_top_users('RTnet_vacc_2020_12_01 - 2021_06_30>20.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80830431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the indegree\n",
    "Distribution_of_the_indegree('RTnet_vacc_2021_06_30.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ffa50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# linguistic analysis #\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4583b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect texts\n",
    "def collect_texts(pkl_dir, fpaths, label):\n",
    "    if label == 'tweets':\n",
    "        df_pro = []\n",
    "        df_anti = []\n",
    "        for f in fpaths:\n",
    "            try:\n",
    "\n",
    "                # read excel of anti and pro nodes\n",
    "                nodes = pd.read_csv('RTnet_vacc_2021_06_30>20.csv')\n",
    "                nodes_of_pro= list(nodes[nodes['modularity_class']==4]['Id'])\n",
    "                nodes_of_anti = list(nodes[nodes['modularity_class']==0]['Id'])\n",
    "\n",
    "                dt = pd.to_datetime(str(f.name)[0:8])\n",
    "                print(dt)\n",
    "                df = pd.read_pickle(f)\n",
    "\n",
    "                # English only\n",
    "                df = df[(df['lang'] == 'en')]\n",
    "\n",
    "                # retweets only\n",
    "                # df = df[df['retweeted_status_created_at'].notnull()]\n",
    "\n",
    "                # replies only\n",
    "                # df = df[df['in_reply_to_user_id_str'].notnull()]\n",
    "\n",
    "                # tweets only\n",
    "                df = df[df['retweeted_status_created_at'].isnull()]\n",
    "                df = df[df['in_reply_to_user_id_str'].isnull()]\n",
    "\n",
    "                # find tweets containing keywords      \n",
    "                df = df[df['text'].str.contains('vaccines|vaccine|vaccinated|vaccination|vaccineoutside|vaccinate|vaccinologist|vaccinert|coronavirusvaccine', na=False)]\n",
    "\n",
    "                # save only time and text\n",
    "                # df = df[['created_at', 'text']]\n",
    "\n",
    "                # save dataframe\n",
    "                for index, row in df.iterrows():\n",
    "                    # sort texts for anti and pro\n",
    "                    if row['user_screen_name'] in nodes_of_anti:\n",
    "                        df_anti.append(row[['created_at', 'text']])\n",
    "                    elif row['user_screen_name'] in nodes_of_pro:\n",
    "                        df_pro.append(row[['created_at', 'text']])\n",
    "            except:\n",
    "                print('error: ', f)\n",
    "        return df_pro, df_anti\n",
    "    elif label == 'replies':\n",
    "        df_pro = []\n",
    "        df_anti = []\n",
    "        for f in fpaths:\n",
    "            try:\n",
    "\n",
    "                # read excel of anti and pro nodes\n",
    "                nodes = pd.read_csv('RTnet_vacc_2021_06_30>20.csv')\n",
    "                nodes_of_pro= list(nodes[nodes['modularity_class']==4]['Id'])\n",
    "                nodes_of_anti = list(nodes[nodes['modularity_class']==0]['Id'])\n",
    "\n",
    "                dt = pd.to_datetime(str(f.name)[0:8])\n",
    "                print(dt)\n",
    "                df = pd.read_pickle(f)\n",
    "\n",
    "                # English only\n",
    "                df = df[(df['lang'] == 'en')]\n",
    "\n",
    "                # retweets only\n",
    "                # df = df[df['retweeted_status_created_at'].notnull()]\n",
    "\n",
    "                # replies only\n",
    "                df = df[df['in_reply_to_user_id_str'].notnull()]\n",
    "\n",
    "                # tweets only\n",
    "                # df = df[df['retweeted_status_created_at'].isnull()]\n",
    "                # df = df[df['in_reply_to_user_id_str'].isnull()]\n",
    "\n",
    "                # find tweets containing keywords      \n",
    "                df = df[df['text'].str.contains('vaccines|vaccine|vaccinated|vaccination|vaccineoutside|vaccinate|vaccinologist|vaccinert|coronavirusvaccine', na=False)]\n",
    "\n",
    "                # save only time and text\n",
    "                # df = df[['created_at', 'text']]\n",
    "\n",
    "                # save dataframe\n",
    "                for index, row in df.iterrows():\n",
    "                    # sort texts for anti and pro\n",
    "                    if row['user_screen_name'] in nodes_of_anti:\n",
    "                        df_anti.append(row[['created_at', 'text']])\n",
    "                    elif row['user_screen_name'] in nodes_of_pro:\n",
    "                        df_pro.append(row[['created_at', 'text']])\n",
    "            except:\n",
    "                print('error: ', f)\n",
    "        return df_pro, df_anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ad125",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "#fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-02-20', '2021-06-30')\n",
    "# collect teeets\n",
    "tweet_pro, tweet_anti = collect_tweets(pkl_dir, fpaths, 'tweets')\n",
    "# collect replies\n",
    "reply_pro, reply_anti = collect_replies(pkl_dir, fpaths, 'replies')\n",
    "\n",
    "# save texts\n",
    "tweet_pro = pd.DataFrame(tweet_pro)\n",
    "tweet_pro.to_csv('tweet_pro')\n",
    "\n",
    "tweet_anti = pd.DataFrame(tweet_anti)\n",
    "tweet_anti.to_csv('tweet_anti')\n",
    "\n",
    "reply_pro = pd.DataFrame(reply_pro)\n",
    "reply_pro.to_csv('reply_pro')\n",
    "\n",
    "reply_anti = pd.DataFrame(reply_anti)\n",
    "reply_anti.to_csv('reply_anti')\n",
    "\n",
    "# save tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d801768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we use LIWC to analyze texts of each group. LIWC can help us get kinds(e.g. negative, positive, analytic) of scores of each text.\n",
    "# We use text's score in each gruop to find if there exist significance difference by independent t-test.\n",
    "# And we conclude each group scores in files 'LIWC2015 Results (tweet_pro.csv).csv', 'LIWC2015 Results (tweet_anti.csv).csv'\n",
    "# 'LIWC2015 Results (reply_pro.csv).csv', 'LIWC2015 Results (reply_anti.csv).csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c314b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate scores results for 'tweet'\n",
    "tweet_pro = pd.read_csv('LIWC2015 Results (tweet_pro.csv).csv')\n",
    "tweet_anti = pd.read_csv('LIWC2015 Results (tweet_anti.csv).csv')\n",
    "tweet_vaccine = tweet_pro.append(tweet_anti).dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c8ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate scores results for 'replies'\n",
    "reply_pro = pd.read_csv('LIWC2015 Results (reply_pro.csv).csv')\n",
    "reply_anti = pd.read_csv('LIWC2015 Results (reply_anti.csv).csv')\n",
    "reply_vaccine = reply_pro.append(reply_anti).dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bc397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(df1, df2, column):\n",
    "    if df1[column].agg(np.std) != df2[column].agg(np.std): \n",
    "        \n",
    "        df1_column = sm.stats.DescrStatsW(df1[column])\n",
    "        df2_column = sm.stats.DescrStatsW(df2[column])\n",
    "        \n",
    "        mean = df2_column.mean - df1_column.mean\n",
    "        pvalue = list(sm.stats.CompareMeans(df2_column, df1_column).ttest_ind(usevar='unequal'))[1]\n",
    "        result.append([mean, pvalue])\n",
    "\n",
    "    else:\n",
    "        print(\"Pooled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d0af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to check the significance difference of means in anti and pro in reply texts and tweet texts respectively\n",
    "\n",
    "# Tweets\n",
    "result = []\n",
    "# categories we want to compare within LIWC\n",
    "categories = ['affect', 'posemo', 'negemo', 'Analytic', 'funct', 'pronoun']\n",
    "for category in categories:\n",
    "    t_test(tweet_pro, tweet_anti, category)\n",
    "\n",
    "# print p-value for each category    \n",
    "k = 0\n",
    "for i in categories:\n",
    "    print(i)\n",
    "    print('p-value: ',result[k][1])\n",
    "    k=k+1 \n",
    "\n",
    "# confidence intervals with bootstrap\n",
    "for category in categories:\n",
    "    print(boot.ci(tweet_pro[category]))\n",
    "    print(boot.ci(tweet_anti[category]))\n",
    "    \n",
    "\n",
    "# Replies\n",
    "result = []\n",
    "# categories we want to compare within LIWC\n",
    "categories = ['affect', 'posemo', 'negemo', 'Analytic', 'funct', 'pronoun']\n",
    "for category in categories:\n",
    "    t_test(reply_pro, reply_anti, category)\n",
    "\n",
    "# print p-value for each category    \n",
    "k = 0\n",
    "for i in categories:\n",
    "    print(i)\n",
    "    print('p-value: ',result[k][1])\n",
    "    k=k+1\n",
    "    \n",
    "# confidence intervals with bootstrap\n",
    "for category in categories:\n",
    "    print(boot.ci(reply_pro[category]))\n",
    "    print(boot.ci(reply_anti[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a530e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want to check if there are significance difference exist in the mode of reply and tweet texts with negative emotion\n",
    "# before and after mass vaccination.\n",
    "\n",
    "# We find the timepoint of 2020-12-01 mannualy and divide all the texts\n",
    "\n",
    "tweet_pro_before = tweet_pro[0:77608]\n",
    "tweet_pro_after = tweet_pro[77608:]\n",
    "\n",
    "tweet_anti_before = tweet_anti[0:25983]\n",
    "tweet_anti_after = tweet_anti[25983:]\n",
    "\n",
    "reply_pro_before = reply_pro[0:7945]\n",
    "reply_pro_after = reply_pro[7945:]\n",
    "\n",
    "reply_anti_before = reply_anti[0:12888]\n",
    "reply_anti_after = reply_anti[12888:]\n",
    "\n",
    "# And we compare all the groups by negetive emotion before and after 2020-12-01\n",
    "\n",
    "# tweet_pro_before VS tweet_anti_before\n",
    "t_test(tweet_pro_before, tweet_anti_before, 'negemo')\n",
    "\n",
    "# tweet_pro_after VS tweet_anti_after\n",
    "t_test(tweet_pro_after, tweet_anti_after, 'negemo')\n",
    "\n",
    "# reply_pro_before VS reply_anti_before\n",
    "t_test(reply_pro_before, reply_anti_before, 'negemo')\n",
    "\n",
    "# tweet_pro_after VS tweet_anti_after\n",
    "t_test(reply_pro_after, reply_anti_after, 'negemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the texts of pro and anti by Tweet and Reply\n",
    "pro = tweet_pro.append(reply_pro)\n",
    "anti = tweet_anti.append(reply_anti)\n",
    "\n",
    "# compare moral perspectives in Reply and Anti\n",
    "moral_name = ['HarmVirtue', \n",
    "       'HarmVice',\n",
    "       'FairnessVirtue',\n",
    "       'FairnessVice',\n",
    "       'IngroupVirtue',\n",
    "       'IngroupVice',\n",
    "       'AuthorityVirtue',\n",
    "       'AuthorityVice',\n",
    "       'PurityVirtue',\n",
    "       'PurityVice',\n",
    "       'MoralityGeneral']\n",
    "\n",
    "# t-test\n",
    "result = []\n",
    "for moral in moral_name:\n",
    "    t_test(pro, anti, moral)\n",
    "    \n",
    "# print p-value for each category    \n",
    "k = 0\n",
    "for i in moral_name:\n",
    "    print(i)\n",
    "    print('p-value: ',result[k][1])\n",
    "    k=k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "594851b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import (WordCloud, get_single_color_func)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SimpleGroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns EXACT colors\n",
    "       to certain words based on the color to words mapping\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.word_to_color = {word: color\n",
    "                              for (color, words) in color_to_words.items()\n",
    "                              for word in words}\n",
    "\n",
    "        self.default_color = default_color\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.word_to_color.get(word, self.default_color)\n",
    "\n",
    "\n",
    "class GroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns DIFFERENT SHADES of\n",
    "       specified colors to certain words based on the color to words mapping.\n",
    "\n",
    "       Uses wordcloud.get_single_color_func\n",
    "\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.color_func_to_words = [\n",
    "            (get_single_color_func(color), set(words))\n",
    "            for (color, words) in color_to_words.items()]\n",
    "\n",
    "        self.default_color_func = get_single_color_func(default_color)\n",
    "\n",
    "    def get_color_func(self, word):\n",
    "        \"\"\"Returns a single_color_func associated with the word\"\"\"\n",
    "        try:\n",
    "            color_func = next(\n",
    "                color_func for (color_func, words) in self.color_func_to_words\n",
    "                if word in words)\n",
    "        except StopIteration:\n",
    "            color_func = self.default_color_func\n",
    "\n",
    "        return color_func\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.get_color_func(word)(word, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63fbc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SpecificWordCloud(name, label):\n",
    "    \n",
    "    if label == 'pro':\n",
    "        # read text\n",
    "        tweet_pro = pd.read_csv('LIWC2015 Results (tweet_pro_df.csv).csv', encoding = 'unicode_escape')\n",
    "        reply_pro = pd.read_csv('LIWC2015 Results (reply_pro_df.csv).csv', encoding = 'unicode_escape')\n",
    "    else:\n",
    "        # read text\n",
    "        tweet_pro = pd.read_csv('LIWC2015 Results (tweet_anti_df.csv).csv', encoding = 'unicode_escape')\n",
    "        reply_pro = pd.read_csv('LIWC2015 Results (reply_anti_df.csv).csv', encoding = 'unicode_escape')\n",
    "        \n",
    "    # read liwc dictionary\n",
    "    liwc_dic = pd.read_csv('liwc dictionary.csv')\n",
    "\n",
    "    # save needed column name\n",
    "    # columns_name = liwc_dic.columns\n",
    "    # name = [i for i in columns_name[30: 35]]\n",
    "\n",
    "    # save needed column \n",
    "    name_dic = liwc_dic[name]\n",
    "\n",
    "    # remove all '*'\n",
    "\n",
    "    def cleanTxt(text):\n",
    "        text = re.sub(r'\\*', '', text)\n",
    "        return text\n",
    "\n",
    "    for i in range(len(name)):\n",
    "        name_dic[name[i]] = name_dic[name[i]].dropna().apply(cleanTxt)\n",
    "\n",
    "    # convert pd to dictionary\n",
    "    name_dic = name_dic.to_dict('list')\n",
    "\n",
    "    # delete nan\n",
    "    clean_dic = {k:[elem for elem in v if elem is not np.nan] for k,v in name_dic.items()}\n",
    "\n",
    "    # create colors for each category\n",
    "    new_key = ['green', 'blue', 'red', 'black', 'orange', 'purple']\n",
    "    old_key = name\n",
    "    new_key = new_key[0: len(name)]\n",
    "    \n",
    "    for i in range(len(new_key)):\n",
    "        clean_dic[new_key[i]] = clean_dic.pop(old_key[i])\n",
    "\n",
    "    # If the value in the dictionary exists in the text, add once for each existence.\n",
    "\n",
    "    # values of tweet_pro \n",
    "    # 'C' is the column of text\n",
    "    tweet_pro_x = []\n",
    "    for i in tweet_pro['C']:\n",
    "        for j in clean_dic.values():\n",
    "            for word in j:\n",
    "                if word in i:\n",
    "                    tweet_pro_x.append(word)\n",
    "\n",
    "    # values of tweet_anti \n",
    "    for i in reply_pro['C']:\n",
    "        for j in clean_dic.values():\n",
    "            for word in j:\n",
    "                if word in i:\n",
    "                    tweet_pro_x.append(word)\n",
    "\n",
    "    # add text\n",
    "    st_tweet_pro_x = ''\n",
    "    for i in tweet_pro_x:\n",
    "        st_tweet_pro_x = st_tweet_pro_x + ' ' + str(i)\n",
    "\n",
    "    # Since the text is small collocations are turned off and text is lower-cased\n",
    "    wc = WordCloud(collocations=False, background_color='white', min_word_length = 3, max_words = 50).generate(st_tweet_pro_x.lower())\n",
    "\n",
    "    color_to_words = clean_dic\n",
    "\n",
    "    # Words that are not in any of the color_to_words values\n",
    "    # will be colored with a grey single color function\n",
    "    default_color = 'grey'\n",
    "\n",
    "    # Create a color function with single tone\n",
    "    # grouped_color_func = SimpleGroupedColorFunc(color_to_words, default_color)\n",
    "\n",
    "    # Create a color function with multiple tones\n",
    "    grouped_color_func = GroupedColorFunc(clean_dic, default_color)\n",
    "\n",
    "    # Apply our color function\n",
    "    wc.recolor(color_func=grouped_color_func)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ecf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloud(index1, index2, label):\n",
    "    # We can specify any pair of numbers correlated with columns\n",
    "    columns_name = liwc_dic.columns\n",
    "    name = [i for i in columns_name[index1: index2]]\n",
    "    print(name)`\n",
    "    SpecificWordCloud(name, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76cab1e-10e5-4f59-b423-4ddd946fde43",
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_dic = pd.read_csv('liwc dictionary.csv')\n",
    "columns_name = liwc_dic.columns\n",
    "print(columns_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f35c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify any pair of numbers correlated with columns to make wordcloud\n",
    "# For example, in LIWC, index 0 to 5 represent 'Insight', 'Cause', 'Discrep', 'Tentat', 'Certain', 'Differ' for Cognitive processes,\n",
    "# so we use wordcloud(0, 6, 'pro') to find if there exist difference in Cognitive processes subcategories by wordcloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
