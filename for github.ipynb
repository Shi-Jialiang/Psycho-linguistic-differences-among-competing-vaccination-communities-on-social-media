{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "828d4fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scikits.bootstrap as boot\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from math import isnan\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import mannwhitneyu\n",
    "import scikits.bootstrap as boot\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# view all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.rcParams['ps.useafm'] = True\n",
    "plt.rcParams['font.sans-serif'] = 'Spica Neue'\n",
    "#plt.rcParams['font.sans-serif'] = 'IPAexGothic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3922e6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# network analysis #\n",
    "####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import file path\n",
    "def dates2fpaths(pkl_dir, start_date_str, end_dates_str):\n",
    "    start_date = pd.to_datetime(start_date_str).tz_localize(None)\n",
    "    end_date = pd.to_datetime(end_dates_str).tz_localize(None)\n",
    "    diff = (end_date - start_date).days + 1\n",
    "    dt_list = [start_date + timedelta(days=i) for i in range(diff)]\n",
    "    return [pkl_dir.joinpath(str(dt)[0:10].replace('-','')+'.pkl.xz') for dt in dt_list]\n",
    "\n",
    "# capture statistic information of vaccine texts\n",
    "def dataset_summary_vaccine(pkl_dir, fpaths):\n",
    "    # initialize variables\n",
    "    tweet_num =0\n",
    "    retweet_num =0\n",
    "    reply_num =0\n",
    "    favorite_count =0\n",
    "    user_name = []\n",
    "    user_hashtag = []\n",
    "    \n",
    "    for f in fpaths:\n",
    "        try:\n",
    "            dt = pd.to_datetime(str(f.name)[0:8])\n",
    "            print(dt)\n",
    "            df = pd.read_pickle(f)\n",
    "\n",
    "            # English only\n",
    "            df = df[(df['lang'] == 'en')] \n",
    "            df = df[df['text'].str.contains('vaccines|vaccine|vaccinated|vaccination|vaccineoutside|vaccinate|vaccinologist|vaccinert|coronavirusvaccine', na=False)]\n",
    "            \n",
    "            # number of retweet\n",
    "            day_retweet = len(df[df['retweeted_status_created_at'].notnull()])\n",
    "            retweet_num = retweet_num + day_retweet\n",
    "            \n",
    "            # number of reply\n",
    "            day_reply = len(df[df['in_reply_to_user_id_str'].notnull()])  \n",
    "            reply_num = reply_num + day_reply           \n",
    "            \n",
    "            # number of tweet\n",
    "            tweet_num = tweet_num + len(df)- day_retweet - day_reply\n",
    "            \n",
    "            # number of favourite\n",
    "            for i in range(len(df['favorite_count'])):\n",
    "                favorite_count += df['favorite_count'][i]\n",
    "            \n",
    "            # all accounts\n",
    "            for i in range(len(df['user_screen_name'])):\n",
    "                user_name.append(df['user_screen_name'][i])\n",
    "            \n",
    "            # all hashtags\n",
    "            for i in df['hashtags']:\n",
    "                # tokenize\n",
    "                tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "                new_words = tokenizer.tokenize(i)\n",
    "                for j in new_words:\n",
    "                    user_hashtag.append(j)\n",
    "                    \n",
    "        except:\n",
    "            print('error: ', f)\n",
    "    return tweet_num, retweet_num, reply_num, favorite_count, user_name, user_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8db9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retweet network\n",
    "# contruct retweet network\n",
    "def retweet_netwroks(pkl_dir, fpaths, filename):\n",
    "\n",
    "    e = []\n",
    "    for f in fpaths:\n",
    "        try:\n",
    "            dt = pd.to_datetime(str(f.name)[0:8])\n",
    "            print(dt)\n",
    "            df = pd.read_pickle(f)\n",
    "\n",
    "            # English only\n",
    "            df = df[(df['lang'] == 'en')] \n",
    "            \n",
    "            # retweets only\n",
    "            df = df[df['retweeted_status_created_at'].notnull()]\n",
    "\n",
    "            # find keyword cocurrence        \n",
    "            df = df[df['text'].str.contains('vaccines|vaccine|vaccinated|vaccination|vaccineoutside|vaccinate|vaccinologist|vaccinert|coronavirusvaccine', na=False)]\n",
    "            \n",
    "            # retweet edges\n",
    "            # from (source) and to (target or user who post the original)\n",
    "            edge_dic = df.groupby(by=['user_screen_name', 'retweeted_status_screen_name']).size().to_dict()\n",
    "            for k, v in edge_dic.items():\n",
    "                e.append([k[0], k[1], {'weight': v}])\n",
    "        except:\n",
    "            print('error: ', f)\n",
    "        \n",
    "    D = nx.from_edgelist(e, create_using=nx.DiGraph)\n",
    "    nx.write_gexf(D, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea65a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Louvain algorithm in Gephi to find Pro and Anti clusters.\n",
    "# Download nodes information of each cluster by Gephi named \"RTnet_vacc_2021_06_30>20.csv\" \n",
    "# which modularity_class of '4' represent for Pro and '0' represent for Anti. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network measures(including basic information of network, Network density, Clustering coefficient and Average distance)\n",
    "def retweet_network_information(name):\n",
    "    # RT network\n",
    "    G = nx.read_gexf(name)\n",
    "    print('All nodes:', nx.info(G))\n",
    "    print('\\n')\n",
    "\n",
    "    # RT network >= 20\n",
    "    node = list(G.nodes)\n",
    "    for i in range(G.number_of_nodes()):\n",
    "        if G.degree[node[i]] < 20:\n",
    "            G.remove_node(node[i])\n",
    "    print('node degree>=20:', nx.info(G))\n",
    "    \n",
    "    # save gexf\n",
    "    # nx.write_gexf(G, 'RTnet_vacc_2020_12_01 - 2021_06_30>20.gexf')\n",
    "\n",
    "    # nodes.csv from Gephi\n",
    "    nodes = pd.read_csv('RTnet_vacc_2021_06_30>20.csv')\n",
    "\n",
    "    # read nodes for Anti and Pro\n",
    "    # 4 represent for Pro, 0 represent for Anti \n",
    "    pro_nodes = list(nodes[nodes['modularity_class']==4]['Id'])\n",
    "    anti_nodes = list(nodes[nodes['modularity_class']==0]['Id'])\n",
    "\n",
    "    print('\\n')\n",
    "    print('number of Pro: ', len(pro_nodes))\n",
    "    print('number of Anti: ',len(anti_nodes))\n",
    "    print('\\n')\n",
    "\n",
    "    # fliter nodes for pro\n",
    "    G_pro = G.subgraph(pro_nodes)\n",
    "    print(nx.info(G_pro))\n",
    "\n",
    "    print('\\n')\n",
    "    # fliter nodes for anti\n",
    "    G_anti = G.subgraph(anti_nodes)\n",
    "    print(nx.info(G_anti))\n",
    "\n",
    "    print('\\n')\n",
    "    # network density\n",
    "    G_network_density = nx.density(G)\n",
    "    print('G_network_density: ', G_network_density)\n",
    "\n",
    "    G_pro_network_density = nx.density(G_pro)\n",
    "    print('G_pro_network_density: ', G_pro_network_density)\n",
    "\n",
    "    G_anti_network_density = nx.density(G_anti)\n",
    "    print('G_anti_network_density: ', G_anti_network_density)\n",
    "    print('\\n')\n",
    "    \n",
    "    # global clustering coeffcient\n",
    "\n",
    "    G_average_clustering = nx.average_clustering(G)\n",
    "    G_pro_average_clustering = nx.average_clustering(G_pro)\n",
    "    G_anti_average_clustering = nx.average_clustering(G_anti)\n",
    "\n",
    "    print('average local clustering coefficient of entirety: ', G_average_clustering)\n",
    "    print('average local clustering coefficient of Pro: ', G_pro_average_clustering)\n",
    "    print('average local clustering coefficient of Anti: ', G_anti_average_clustering)\n",
    "    print('\\n')\n",
    "    \n",
    "    # average distance\n",
    "    G_pro_average_distance = nx.average_shortest_path_length(nx.to_undirected(G_pro))\n",
    "    G_anti_average_distance = nx.average_shortest_path_length(nx.to_undirected(G_anti))\n",
    "\n",
    "    print('Average distance of Pro: ', G_pro_average_distance)\n",
    "    print('Average distance of Anti: ', G_anti_average_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858f74e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Top-10 users \n",
    "def find_top_users(name):\n",
    "    G = nx.read_gexf(name)\n",
    "    a = G.degree\n",
    "    degree = sorted(G.degree(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "    def find_top_users_for_cluster(number):\n",
    "\n",
    "        node  = list(nodes[nodes['modularity_class']==number]['Id'])\n",
    "        num = len(list(nodes[nodes['modularity_class']==number]['Id']))\n",
    "\n",
    "        # names and degrees of all nodes labeled with 'number'\n",
    "        degree0  = []\n",
    "        for i in range(len(degree)):\n",
    "            if degree[i][0] in node:\n",
    "                degree0.append(degree[i])\n",
    "        ratio = num/len(nodes)*100\n",
    "        ratio=('%.2f' % ratio)\n",
    "\n",
    "        # 打印top 10 \n",
    "        top_10 = sorted(degree0, key=lambda x:x[1], reverse=True)[:10]\n",
    "        print(\"Top 10 of {}:\".format(number))\n",
    "        # print('Ratio:{}%'.format(ratio))\n",
    "        print(top_10)  \n",
    "\n",
    "    # print ratio and top user of each cluster\n",
    "    cluster_name = [4,0]\n",
    "    for i in cluster_name:\n",
    "        find_top_users_for_cluster(i)\n",
    "        print('---------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf8821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revising...\n",
    "def Distribution_of_the_indegree(G):\n",
    "    \n",
    "    # RT network\n",
    "    G = nx.read_gexf(name)\n",
    "\n",
    "    # nodes.csv from Gephi\n",
    "    nodes = pd.read_csv('RTnet_vacc_2021_06_30>20.csv')\n",
    "\n",
    "    # read nodes for Anti and Pro\n",
    "    # 4 represent for Pro, 0 represent for Anti \n",
    "    pro_nodes = list(nodes[nodes['modularity_class']==4]['Id'])\n",
    "    anti_nodes = list(nodes[nodes['modularity_class']==0]['Id'])\n",
    "    \n",
    "    degree_hist = np.array(nx.degree_histogram(G))\n",
    "    pk = degree_hist / degree_hist.sum()\n",
    "    plt.xlabel('In Degree of G')\n",
    "    plt.ylabel('Fraction of Nodes')\n",
    "    plt.loglog(pk)\n",
    "    #plt.xlim(1, 11000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bf9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "#fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-02-20', '2021-06-30')\n",
    "# capture statistic information of vaccine\n",
    "tweet_num_vaccine, retweet_num_vaccine, reply_num_vaccine, favorite_count_vaccine, user_name_vaccine, user_hashtag_vaccine = dataset_summary_vaccine(pkl_dir, fpaths)\n",
    "print(tweet_num_vaccine, retweet_num_vaccine, reply_num_vaccine, favorite_count_vaccine, len(set(user_name_vaccine)), len(set(user_hashtag_vaccine)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9989d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save retweet network\n",
    "\n",
    "# contruct retweet network\n",
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "# fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-02-20', '2021-06-30')\n",
    "retweet_netwroks(pkl_dir, fpaths, 'RTnet_vacc_2021_06_30>20.gexf')\n",
    "\n",
    "# contruct retweet network before mass vaccination\n",
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "#fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-02-20', '2020-12-01')\n",
    "retweet_netwroks(pkl_dir, fpaths, 'RTnet_vacc_2020_02_20 - 2020_12_01.gexf')\n",
    "\n",
    "# contruct retweet network after mass vaccination\n",
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "#fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-12-02', '2021-06-30')\n",
    "retweet_netwroks(pkl_dir, fpaths, 'RTnet_vacc_2020_12_01 - 2021_06_30.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0eec3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measures and find Top-10 users for different retweet network which 2020_12_01 represent the timepoint for mass vaccination \n",
    "# 2020_02_20 To 2021_06_30\n",
    "print('Timeline: From 2020_02_20 To 2021_06_30')\n",
    "retweet_network_information('RTnet_vacc_2021_06_30.gexf')\n",
    "find_top_users('RTnet_vacc_2021_06_30>20.gexf')\n",
    "\n",
    "# 2020_02_20 To 2020_12_01\n",
    "print('Timeline: From 2020_02_20 To 2020_12_01')\n",
    "retweet_network_information('RTnet_vacc_2020_02_20 - 2020_12_01.gexf')\n",
    "find_top_users('RTnet_vacc_2020_02_20 - 2020_12_01>20.gexf')\n",
    "\n",
    "# 2020_12_01 To 2021_06_30\n",
    "print('Timeline: From 2020_12_01 To 2021_06_30')\n",
    "retweet_network_information('RTnet_vacc_2020_12_01 - 2021_06_30.gexf')\n",
    "find_top_users('RTnet_vacc_2020_12_01 - 2021_06_30>20.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d4f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revising...\n",
    "# Distribution of the indegree\n",
    "Distribution_of_the_indegree('RTnet_vacc_2021_06_30.gexf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98756eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# linguistic analysis #\n",
    "#######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4acfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect texts\n",
    "def collect_texts(pkl_dir, fpaths, label):\n",
    "    if label == 'tweets':\n",
    "        df_pro = []\n",
    "        df_anti = []\n",
    "        for f in fpaths:\n",
    "            try:\n",
    "\n",
    "                # read excel of anti and pro nodes\n",
    "                nodes = pd.read_csv('RTnet_vacc_2021_06_30>20.csv')\n",
    "                nodes_of_pro= list(nodes[nodes['modularity_class']==4]['Id'])\n",
    "                nodes_of_anti = list(nodes[nodes['modularity_class']==0]['Id'])\n",
    "\n",
    "                dt = pd.to_datetime(str(f.name)[0:8])\n",
    "                print(dt)\n",
    "                df = pd.read_pickle(f)\n",
    "\n",
    "                # English only\n",
    "                df = df[(df['lang'] == 'en')]\n",
    "\n",
    "                # retweets only\n",
    "                # df = df[df['retweeted_status_created_at'].notnull()]\n",
    "\n",
    "                # replies only\n",
    "                # df = df[df['in_reply_to_user_id_str'].notnull()]\n",
    "\n",
    "                # tweets only\n",
    "                df = df[df['retweeted_status_created_at'].isnull()]\n",
    "                df = df[df['in_reply_to_user_id_str'].isnull()]\n",
    "\n",
    "                # find tweets containing keywords      \n",
    "                df = df[df['text'].str.contains('vaccines|vaccine|vaccinated|vaccination|vaccineoutside|vaccinate|vaccinologist|vaccinert|coronavirusvaccine', na=False)]\n",
    "\n",
    "                # save only time and text\n",
    "                # df = df[['created_at', 'text']]\n",
    "\n",
    "                # save dataframe\n",
    "                for index, row in df.iterrows():\n",
    "                    # sort texts for anti and pro\n",
    "                    if row['user_screen_name'] in nodes_of_anti:\n",
    "                        df_anti.append(row[['created_at', 'text']])\n",
    "                    elif row['user_screen_name'] in nodes_of_pro:\n",
    "                        df_pro.append(row[['created_at', 'text']])\n",
    "            except:\n",
    "                print('error: ', f)\n",
    "        return df_pro, df_anti\n",
    "    elif label == 'replies':\n",
    "        df_pro = []\n",
    "        df_anti = []\n",
    "        for f in fpaths:\n",
    "            try:\n",
    "\n",
    "                # read excel of anti and pro nodes\n",
    "                nodes = pd.read_csv('RTnet_vacc_2021_06_30>20.csv')\n",
    "                nodes_of_pro= list(nodes[nodes['modularity_class']==4]['Id'])\n",
    "                nodes_of_anti = list(nodes[nodes['modularity_class']==0]['Id'])\n",
    "\n",
    "                dt = pd.to_datetime(str(f.name)[0:8])\n",
    "                print(dt)\n",
    "                df = pd.read_pickle(f)\n",
    "\n",
    "                # English only\n",
    "                df = df[(df['lang'] == 'en')]\n",
    "\n",
    "                # retweets only\n",
    "                # df = df[df['retweeted_status_created_at'].notnull()]\n",
    "\n",
    "                # replies only\n",
    "                df = df[df['in_reply_to_user_id_str'].notnull()]\n",
    "\n",
    "                # tweets only\n",
    "                # df = df[df['retweeted_status_created_at'].isnull()]\n",
    "                # df = df[df['in_reply_to_user_id_str'].isnull()]\n",
    "\n",
    "                # find tweets containing keywords      \n",
    "                df = df[df['text'].str.contains('vaccines|vaccine|vaccinated|vaccination|vaccineoutside|vaccinate|vaccinologist|vaccinert|coronavirusvaccine', na=False)]\n",
    "\n",
    "                # save only time and text\n",
    "                # df = df[['created_at', 'text']]\n",
    "\n",
    "                # save dataframe\n",
    "                for index, row in df.iterrows():\n",
    "                    # sort texts for anti and pro\n",
    "                    if row['user_screen_name'] in nodes_of_anti:\n",
    "                        df_anti.append(row[['created_at', 'text']])\n",
    "                    elif row['user_screen_name'] in nodes_of_pro:\n",
    "                        df_pro.append(row[['created_at', 'text']])\n",
    "            except:\n",
    "                print('error: ', f)\n",
    "        return df_pro, df_anti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d06421",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_dir = Path('CoronaTweets_pkl_20210630')\n",
    "#fpaths = sorted(pkl_dir.glob('*.pkl.xz'))\n",
    "fpaths = dates2fpaths(pkl_dir, '2020-02-20', '2021-06-30')\n",
    "# collect teeets\n",
    "tweet_pro, tweet_anti = collect_tweets(pkl_dir, fpaths, 'tweets')\n",
    "# collect replies\n",
    "reply_pro, reply_anti = collect_replies(pkl_dir, fpaths, 'replies')\n",
    "\n",
    "# save texts\n",
    "tweet_pro = pd.DataFrame(tweet_pro)\n",
    "tweet_pro.to_csv('tweet_pro')\n",
    "\n",
    "tweet_anti = pd.DataFrame(tweet_anti)\n",
    "tweet_anti.to_csv('tweet_anti')\n",
    "\n",
    "reply_pro = pd.DataFrame(reply_pro)\n",
    "reply_pro.to_csv('reply_pro')\n",
    "\n",
    "reply_anti = pd.DataFrame(reply_anti)\n",
    "reply_anti.to_csv('reply_anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we use LIWC to analyze texts of each group. LIWC can help us get kinds(e.g. negative, positive, analytic) of scores of each text.\n",
    "# We use text's score in each gruop to find if there exist significance difference by independent t-test.\n",
    "# And we conclude each group scores in files 'LIWC2015 Results (tweet_pro.csv).csv', 'LIWC2015 Results (tweet_anti.csv).csv'\n",
    "# 'LIWC2015 Results (reply_pro.csv).csv', 'LIWC2015 Results (reply_anti.csv).csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a360794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate scores results for 'tweet'\n",
    "tweet_pro = pd.read_csv('LIWC2015 Results (tweet_pro.csv).csv')\n",
    "tweet_anti = pd.read_csv('LIWC2015 Results (tweet_anti.csv).csv')\n",
    "tweet_vaccine = tweet_pro.append(tweet_anti).dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2135e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate scores results for 'replies'\n",
    "reply_pro = pd.read_csv('LIWC2015 Results (reply_pro.csv).csv')\n",
    "reply_anti = pd.read_csv('LIWC2015 Results (reply_anti.csv).csv')\n",
    "reply_vaccine = reply_pro.append(reply_anti).dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cb57d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(df1, df2, column):\n",
    "    if df1[column].agg(np.std) != df2[column].agg(np.std): \n",
    "        \n",
    "        df1_column = sm.stats.DescrStatsW(df1[column])\n",
    "        df2_column = sm.stats.DescrStatsW(df2[column])\n",
    "        \n",
    "        mean = df2_column.mean - df1_column.mean\n",
    "        pvalue = list(sm.stats.CompareMeans(df2_column, df1_column).ttest_ind(usevar='unequal'))[1]\n",
    "        result.append([mean, pvalue])\n",
    "\n",
    "    else:\n",
    "        print(\"Pooled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fc6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to check the significance difference of means in anti and pro in reply texts and tweet texts respectively\n",
    "\n",
    "# Tweets\n",
    "result = []\n",
    "# categories we want to compare within LIWC\n",
    "categories = ['affect', 'posemo', 'negemo', 'Analytic', 'funct', 'pronoun']\n",
    "for category in categories:\n",
    "    t_test(tweet_pro, tweet_anti, category)\n",
    "\n",
    "# print p-value for each category    \n",
    "k = 0\n",
    "for i in categories:\n",
    "    print(i)\n",
    "    print('p-value: ',result[k][1])\n",
    "    k=k+1 \n",
    "\n",
    "# confidence intervals with bootstrap\n",
    "for category in categories:\n",
    "    print(boot.ci(tweet_pro[category]))\n",
    "    print(boot.ci(tweet_anti[category]))\n",
    "    \n",
    "\n",
    "# Replies\n",
    "result = []\n",
    "# categories we want to compare within LIWC\n",
    "categories = ['affect', 'posemo', 'negemo', 'Analytic', 'funct', 'pronoun']\n",
    "for category in categories:\n",
    "    t_test(reply_pro, reply_anti, category)\n",
    "\n",
    "# print p-value for each category    \n",
    "k = 0\n",
    "for i in categories:\n",
    "    print(i)\n",
    "    print('p-value: ',result[k][1])\n",
    "    k=k+1\n",
    "    \n",
    "# confidence intervals with bootstrap\n",
    "for category in categories:\n",
    "    print(boot.ci(reply_pro[category]))\n",
    "    print(boot.ci(reply_anti[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also want to check if there are significance difference exist in the mode of reply and tweet texts with negative emotion\n",
    "# before and after mass vaccination.\n",
    "\n",
    "# We find the timepoint of 2020-12-01 mannualy and divide all the texts\n",
    "\n",
    "tweet_pro_before = tweet_pro[0:77608]\n",
    "tweet_pro_after = tweet_pro[77608:]\n",
    "\n",
    "tweet_anti_before = tweet_anti[0:25983]\n",
    "tweet_anti_after = tweet_anti[25983:]\n",
    "\n",
    "reply_pro_before = reply_pro[0:7945]\n",
    "reply_pro_after = reply_pro[7945:]\n",
    "\n",
    "reply_anti_before = reply_anti[0:12888]\n",
    "reply_anti_after = reply_anti[12888:]\n",
    "\n",
    "# And we compare all the groups by negetive emotion before and after 2020-12-01\n",
    "\n",
    "# tweet_pro_before VS tweet_anti_before\n",
    "t_test(tweet_pro_before, tweet_anti_before, 'negemo')\n",
    "\n",
    "# tweet_pro_after VS tweet_anti_after\n",
    "t_test(tweet_pro_after, tweet_anti_after, 'negemo')\n",
    "\n",
    "# reply_pro_before VS reply_anti_before\n",
    "t_test(reply_pro_before, reply_anti_before, 'negemo')\n",
    "\n",
    "# tweet_pro_after VS tweet_anti_after\n",
    "t_test(reply_pro_after, reply_anti_after, 'negemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the texts of pro and anti by Tweet and Reply\n",
    "pro = tweet_pro.append(reply_pro)\n",
    "anti = tweet_anti.append(reply_anti)\n",
    "\n",
    "# compare moral perspectives in Reply and Anti\n",
    "moral_name = ['HarmVirtue', \n",
    "       'HarmVice',\n",
    "       'FairnessVirtue',\n",
    "       'FairnessVice',\n",
    "       'IngroupVirtue',\n",
    "       'IngroupVice',\n",
    "       'AuthorityVirtue',\n",
    "       'AuthorityVice',\n",
    "       'PurityVirtue',\n",
    "       'PurityVice',\n",
    "       'MoralityGeneral']\n",
    "\n",
    "# t-test\n",
    "result = []\n",
    "for moral in moral_name:\n",
    "    t_test(pro, anti, moral)\n",
    "    \n",
    "# print p-value for each category    \n",
    "k = 0\n",
    "for i in moral_name:\n",
    "    print(i)\n",
    "    print('p-value: ',result[k][1])\n",
    "    k=k+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
